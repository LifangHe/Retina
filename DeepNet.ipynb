{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 28, 28, 1), (200000, 10), <type 'numpy.ndarray'>, <type 'numpy.ndarray'>)\n",
      "('Validation set', (10000, 28, 28, 1), (10000, 10), <type 'numpy.ndarray'>, <type 'numpy.ndarray'>)\n",
      "('Test set', (18724, 28, 28, 1), (18724, 10), <type 'numpy.ndarray'>, <type 'numpy.ndarray'>)\n",
      "('train_labels.shape[0]', 200000)\n",
      "defined accuracy function\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape, type(train_dataset), type(train_labels))\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape, type(valid_dataset), type(valid_labels))\n",
    "print('Test set', test_dataset.shape, test_labels.shape, type(test_dataset), type(test_labels))\n",
    "print('train_labels.shape[0]', train_labels.shape[0])\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "print(\"defined accuracy function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape [24, 28, 28, 1]\n",
      "conv0 [24, 28, 28, 18]\n",
      "conv1 [24, 26, 26, 18]\n",
      "conv2 [24, 12, 12, 18]\n",
      "Nodes created over computation graph\n",
      "notice how in this architecture, learning seems to take longer to show increase in accuracy\n",
      "at step 18000 and LR 0.018 finally the accuracy jumps from 10.4% which is almost random to  \n",
      " 73.9% which is a 64% change in validation accuracy\n"
     ]
    }
   ],
   "source": [
    "batch_size = 24\n",
    "patch_size = 3\n",
    "depth = 18\n",
    "num_hidden = 128\n",
    "L2_reg_const = 0.5e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  dropout_p = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  layer0_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer0_biases = tf.Variable(tf.zeros([depth]))\n",
    "\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([12*12* depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_hidden], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "  layer5_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "  layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "  L2_reg = (tf.nn.l2_loss(layer0_weights) + tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer2_weights)\n",
    "            + tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer4_weights)  + tf.nn.l2_loss(layer5_weights))  \n",
    "\n",
    "  # Model.\n",
    "  def model(data, dropout_p, verbose = False):\n",
    "    \n",
    "    conv0 = tf.nn.relu((tf.nn.conv2d(data,layer0_weights,[1, 1, 1, 1],padding='SAME'))+layer0_biases)\n",
    "    \n",
    "    conv1 = tf.nn.relu((tf.nn.conv2d(conv0,layer1_weights,[1, 1, 1, 1],padding='VALID'))+layer1_biases)\n",
    "    \n",
    "    conv2 = tf.nn.relu((tf.nn.conv2d(conv1, layer2_weights, [1, 2, 2, 1], padding='VALID')) + layer2_biases)\n",
    "    \n",
    "    shape = conv2.get_shape().as_list() \n",
    "       \n",
    "    reshape2 = tf.reshape(conv2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    hidden3 = tf.nn.relu(tf.matmul(reshape2, layer3_weights) + layer3_biases)\n",
    "    \n",
    "    layer_dropout3 = tf.nn.dropout(hidden3, dropout_p)\n",
    "    \n",
    "    hidden4 = tf.nn.relu(tf.matmul(layer_dropout3, layer4_weights) + layer4_biases)\n",
    "    \n",
    "    final_output = tf.matmul(hidden4, layer5_weights) + layer5_biases\n",
    "    \n",
    "    if verbose == True:\n",
    "        print 'data shape', data.get_shape().as_list() # [16, 28, 28, 1]\n",
    "        print 'conv0', conv0.get_shape().as_list()\n",
    "        print 'conv1', conv1.get_shape().as_list()\n",
    "        print 'conv2', conv2.get_shape().as_list()\n",
    "    \n",
    "    return final_output\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, dropout_p)\n",
    "  loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + L2_reg_const*L2_reg )\n",
    "    \n",
    "  # Optimizer.\n",
    "\n",
    "  global_step = tf.Variable(0, trainable=False)\n",
    "  starter_learning_rate = 0.1\n",
    "  learning_rate = tf.train.exponential_decay(starter_learning_rate,global_step,100,0.99,staircase=True)\n",
    "\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(model(tf_train_dataset, dropout_p, verbose = True))\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset, dropout_p = 1.0))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset, dropout_p = 1.0))\n",
    "\n",
    "print 'Nodes created over computation graph'\n",
    "print 'notice how in this architecture, learning seems to take longer to show increase in accuracy'\n",
    "print 'at step 18000 and LR 0.018 finally the accuracy jumps from 10.4% which is almost random to  '\n",
    "print ' 73.9% which is a 64% change in validation accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "('learning rate', 0.1)\n",
      "Minibatch training set loss at step 0: 8.408677\n",
      "Minibatch training set accuracy: 12.5%\n",
      "Validation accuracy: 10.1%\n",
      "('learning rate', 0.098999999)\n",
      "Minibatch training set loss at step 100: 2.379555\n",
      "Minibatch training set accuracy: 75.0%\n",
      "Validation accuracy: 63.2%\n",
      "('learning rate', 0.098010004)\n",
      "Minibatch training set loss at step 200: 1.694413\n",
      "Minibatch training set accuracy: 75.0%\n",
      "Validation accuracy: 73.5%\n",
      "('learning rate', 0.097029902)\n",
      "Minibatch training set loss at step 300: 1.737690\n",
      "Minibatch training set accuracy: 79.2%\n",
      "Validation accuracy: 76.0%\n",
      "('learning rate', 0.096059605)\n",
      "Minibatch training set loss at step 400: 1.866930\n",
      "Minibatch training set accuracy: 66.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2400\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  start_time = timeit.default_timer()\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict_train = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, dropout_p: 0.5}\n",
    "        \n",
    "    _, training_loss, LR = session.run([optimizer, loss, learning_rate], feed_dict=feed_dict_train)\n",
    "    \n",
    "    feed_train_predict = {tf_train_dataset : batch_data, dropout_p: 1}\n",
    "        \n",
    "    batch_predictions = session.run(train_prediction, feed_dict=feed_train_predict)\n",
    "    \n",
    "    if (step % 100 == 0): \n",
    "       print('learning rate',LR)\n",
    "       print(\"Minibatch training set loss at step %d: %f\" % (step, training_loss))\n",
    "       print(\"Minibatch training set accuracy: %.1f%%\" % accuracy(batch_predictions, batch_labels))\n",
    "       print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  end_time = timeit.default_timer()\n",
    "  #feed_test_predict = {tf_test_dataset : test_dataset, dropout_p: 1}\n",
    "  #test_predictions = session.run(test_prediction, feed_dict=feed_test_predict)\n",
    "  print(\"Finished: Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  print('ran for %.2fm' % ((end_time - start_time) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
